{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## m365 Data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "storage_account = 'steduanalyticscisda1'\n",
        "\n",
        "stage1 = f'abfss://stage1@{storage_account}.dfs.core.windows.net'\n",
        "stage2 = f'abfss://stage2@{storage_account}.dfs.core.windows.net'\n",
        "stage3 = f'abfss://stage3@{storage_account}.dfs.core.windows.net'\n",
        "\n",
        "stage1_m365 = f'{stage1}/m365/DIPData'\n",
        "stage1_m365_activity = f'{stage1}/m365/DIPData/Activity/ApplicationUsage'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Roster data from stage 1 to stage 2\n",
        "#\n",
        "# Sets up the edu_dl (stage 2 data lake) with whatever data is found in the DIP inbound folder.\n",
        "# This includes:\n",
        "# - adding column names\n",
        "# - casting values into a schema\n",
        "\n",
        "# Calendar\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Calendar.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Calendar')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Description, cast(_c3 as int) SchoolYear, cast(_c4 as boolean) IsCurrent, _c5 ExternalId, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c7, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c8 as boolean) IsActive, _c9 OrgId from Calendar\")\n",
        "  df.write.format(\"parquet\").mode(\"overwrite\").save(f'{stage2}/m365/Calendar')\n",
        "# Course\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Course.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Course')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Code, _c3 Description, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CalendarId from Course\")\n",
        "  df.write.format(\"parquet\").mode(\"overwrite\").save(f'{stage2}/m365/Course')\n",
        "# Org\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Org.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Org')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Identifier, _c3 ExternalId, to_timestamp(_c4, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c6 as boolean) IsActive, _c7 ParentOrgId, _c8 RefOrgTypeId, _c9 SourceSystemId from Org\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/Org')\n",
        "# Person\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Person.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Person')\n",
        "  df_Person = spark.sql(\"select _c0 Id, _c1 FirstName, _c2 MiddleName, _c3 LastName, _c4 GenerationCode, _c5 Prefix, _c6 EnabledUser, _c7 ExternalId, to_timestamp(_c8, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c9, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c10 as boolean) IsActive, _c11 SourceSystemId from Person\")\n",
        "  df_Person.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/Person')\n",
        "# PersonIdentifier\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/PersonIdentifier.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'PersonIdentifier')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Identifier, _c2 Description, _c3 RefIdentifierTypeId, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 PersonId, _c9 SourceSystemId from PersonIdentifier\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/PersonIdentifier')\n",
        "# RefDefinition\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/RefDefinition.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'RefDefinition')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 RefType, _c2 Namespace, _c3 Code, cast(_c4 as int) SortOrder, _c5 Description, cast(_c6 as boolean) IsActive from RefDefinition\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/RefDefinition')\n",
        "# Section\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Section.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Section')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Code, _c3 Location, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CourseId, _c9 RefSectionTypeId, _c10 SessionId, _c11 OrgId from Section\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/Section')\n",
        "# Session\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Session.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Session')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') BeginDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') EndDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CalendarId, _c9 ParentSessionId, _c10 RefSessionTypeId from Session\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/Session')\n",
        "# StaffOrgAffiliation\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StaffOrgAffiliation.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StaffOrgAffiliation')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimary, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 OrgId, _c9 PersonId, _c10 RefStaffOrgRoleId from StaffOrgAffiliation\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/StaffOrgAffiliation')\n",
        "# StaffSectionMembership\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StaffSectionMembership.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StaffSectionMembership')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimaryStaffForSection, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 PersonId, _c9 RefStaffSectionRoleId, _c10 SectionId from StaffSectionMembership\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/StaffSectionMembership')\n",
        "# StudentOrgAffiliation\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StudentOrgAffiliation.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StudentOrgAffiliation')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimary, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 OrgId, _c9 PersonId, _c10 RefGradeLevelId, _c11 RefStudentOrgRoleId, _c12 RefEnrollmentStatusId from StudentOrgAffiliation\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/StudentOrgAffiliation')\n",
        "# StudentSectionMembership\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StudentSectionMembership.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StudentSectionMembership')\n",
        "  df = spark.sql(\"select _c0 Id, to_timestamp(_c1, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c3 ExternalId, to_timestamp(_c4, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c6 as boolean) IsActive, _c7 PersonId, _c8 RefGradeLevelWhenCourseTakenId, _c9 RefStudentSectionRoleId, _c10 SectionId from StudentSectionMembership\")\n",
        "  df.write.format('parquet').mode('overwrite').save(f'{stage2}/m365/StudentSectionMembership')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Activity data from stage1 into stage2.\n",
        "#\n",
        "# If this is the first load, it loads all activity data.\n",
        "# If this is a subsequent load, it determines the max date currently stored and only loads data from after that date.\n",
        "\n",
        "def append_to_activity_table(max_date=False):\n",
        "    df = spark.read.csv(stage1_m365_activity, header='false') \n",
        "    sqlContext.registerDataFrameAsTable(df, 'Activity')\n",
        "    df_Activity = spark.sql(\"select to_timestamp(_c0) BinDate, _c1 Upn, _c2 UserId, _c3 Application, cast(_c4 as int) SumNumberOfSignals, _c5 Client, cast(_c6 as int) Duration, _c7 LearningActivity, '' PersonId from Activity\")\n",
        "    \n",
        "    if (max_date):\n",
        "        df_Activity = df_Activity.filter(df_Activity.BinDate > max_date)\n",
        "\n",
        "    if (df_Activity.count() == 0):\n",
        "        print('No new activity data to load')\n",
        "    else:\n",
        "        print('Adding activity data later than: ' + str(max_date))\n",
        "        # The assumption here is that there will always be data in these inbound files\n",
        "        sqlContext.registerDataFrameAsTable(df_Activity, 'Activity')\n",
        "        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/PersonIdentifier'), 'PersonIdentifier')\n",
        "        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/RefDefinition'), 'RefDefinition')\n",
        "\n",
        "        df1 = spark.sql( \\\n",
        "        \"select act.BinDate, act.Upn, act.UserId, act.Application, act.SumNumberOfSignals, act.Client, act.Duration, act.LearningActivity, pi.PersonId \\\n",
        "        from PersonIdentifier pi, RefDefinition rd, Activity act \\\n",
        "        where \\\n",
        "            pi.RefIdentifierTypeId = rd.Id \\\n",
        "            and rd.RefType = 'RefIdentifierType' \\\n",
        "            and rd.Code = 'username' \\\n",
        "            and pi.Identifier = act.Upn \\\n",
        "            and act.Upn <> '' \\\n",
        "        \")\n",
        "\n",
        "        df2 = spark.sql( \\\n",
        "        \"select act.BinDate, act.Upn, act.UserId, act.Application, act.SumNumberOfSignals, act.Client, act.Duration, act.LearningActivity, pi.PersonId \\\n",
        "        from PersonIdentifier pi, RefDefinition rd, Activity act\\\n",
        "        where \\\n",
        "            pi.RefIdentifierTypeId = rd.Id\\\n",
        "            and rd.RefType = 'RefIdentifierType'\\\n",
        "            and rd.Code = 'ActiveDirectoryId'\\\n",
        "            and pi.Identifier = act.UserId\\\n",
        "            and act.UserId is not null\\\n",
        "        \")\n",
        "\n",
        "        df1.write.format(\"parquet\").mode(\"append\").save(f'{stage2}/m365/Activity')\n",
        "        df2.write.format(\"parquet\").mode(\"append\").save(f'{stage2}/m365/Activity')\n",
        "\n",
        "try:\n",
        "    df = spark.read.format('parquet').load(f'{stage2}/m365/Activity')\n",
        "    max_date = df.agg({'BinDate': 'max'}).first()[0]\n",
        "    print(max_date)\n",
        "    append_to_activity_table(max_date)\n",
        "except:\n",
        "    print(\"No Activity data has been loaded into stage2 data lake yet.\")\n",
        "    append_to_activity_table()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Anonymize the data from edu_dl (stage2) and load into anon_edu_dl (stage3)\n",
        "# - redact columns from Person table\n",
        "# - apply a hash to every occurrence of PersonId\n",
        "# - redact UPN and UserId from Activity table\n",
        "# - don't bring in PersonIdentifier, Student, Staff and other tables not needed or empty (some tables are not being populated by EDP)\n",
        "\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "# Activity\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/Activity')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('Upn', lit('*')).withColumn('UserId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Activity')\n",
        "# Calendar, Course, Org\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Calendar').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Calendar')\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Course').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Course')\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Org').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Org')\n",
        "# Person\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/Person')\n",
        "df = df.withColumn('Id', sha2(df.Id, 256)).withColumn('FirstName', lit('*')).withColumn(\"MiddleName\", lit('*')).withColumn('LastName', lit('*')).withColumn('ExternalId', sha2(df.ExternalId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Person')\n",
        "# PersonIdentifier\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/PersonIdentifier')\n",
        "df = df.withColumn('PersonId', sha2(df.Id, 256)).withColumn('Identifier', lit('*')).withColumn(\"ExternalId\", lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/PersonIdentifier')\n",
        "# RefDefinition, Section, Session\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/RefDefinition').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/RefDefinition')\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Section').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Section')\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Session').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Session')\n",
        "# StaffOrgAffiliation\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/StaffOrgAffiliation')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/StaffOrgAffiliation')\n",
        "# StaffSectionMembership\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/StaffSectionMembership')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/StaffSectionMembership')\n",
        "# StudentOrgAffiliation\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/StudentOrgAffiliation')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/StudentOrgAffiliation')\n",
        "# StudentSectionMembership\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/StudentSectionMembership')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/StudentSectionMembership')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
        "    spark.sql(f\"create table if not exists {db_name}.Activity using PARQUET location '{source_path}/Activity'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Calendar using PARQUET location '{source_path}/Calendar'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Course using PARQUET location '{source_path}/Course'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Org using PARQUET location '{source_path}/Org'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Person using PARQUET location '{source_path}/Person'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.PersonIdentifier using PARQUET location '{source_path}/PersonIdentifier'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.RefDefinition using PARQUET location '{source_path}/RefDefinition'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Section using PARQUET location '{source_path}/Section'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Session using PARQUET location '{source_path}/Session'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StaffOrgAffiliation using PARQUET location '{source_path}/StaffOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StaffSectionMembership using PARQUET location '{source_path}/StaffSectionMembership'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StudentOrgAffiliation using PARQUET location '{source_path}/StudentOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StudentSectionMembership using PARQUET location '{source_path}/StudentSectionMembership'\")\n",
        "# Drop all tables in a db, then drop the db\n",
        "def drop_db(db_name):\n",
        "    df = spark.sql('SHOW TABLES FROM ' + db_name)\n",
        "    for row in df.rdd.collect():\n",
        "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{row['tableName']}\")\n",
        "    spark.sql(f\"DROP DATABASE {db_name}\")    \n",
        "\n",
        "#drop_db('s2_m365')\n",
        "create_spark_db('s2_m365', stage2 + '/m365')\n",
        "#drop_db('s3_m365')\n",
        "create_spark_db('s3_m365', stage3 + '/m365')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "sessionOptions": {
      "driverMemory": "28g",
      "driverCores": 4,
      "executorMemory": "28g",
      "executorCores": 4,
      "numExecutors": 2,
      "keepAliveTimeout": 30,
      "conf": {
        "spark.dynamicAllocation.enabled": "false",
        "spark.dynamicAllocation.minExecutors": "2",
        "spark.dynamicAllocation.maxExecutors": "2"
      }
    },
    "saveOutput": true,
    "language_info": {
      "name": "scala",
      "version": "2.11.12",
      "mimetype": "text/x-scala",
      "file_extension": ".scala",
      "pygments_lexer": "scala",
      "codemirror_mode": "scala",
      "nbconvert_exporter": "scala"
    },
    "a365ComputeOptions": {
      "nodeSize": "Small",
      "auth": {
        "authResource": "https://dev.azuresynapse.net",
        "type": "AAD"
      },
      "name": "spark1",
      "nodeCount": 10,
      "endpoint": "https://syeduanalyticscisda1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark1",
      "automaticScaleJobs": false,
      "type": "Spark",
      "id": "/subscriptions/17be3a95-2f9c-4c24-80cc-3974d6021e31/resourceGroups/EduAnalyticsa1/providers/Microsoft.Synapse/workspaces/syeduanalyticscisda1/bigDataPools/spark1",
      "sparkVersion": "2.4",
      "extraHeader": null
    },
    "microsoft": {
      "language": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}